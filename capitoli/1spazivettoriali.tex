\chapter{Spazi Vettoriali}

\begin{definition}{(vettori e spazi vettoriali)}
I vettori sono, essenzialmente, oggetti che si possono "sommare e moltiplicare per scalari". Formalmente uno spazio vettoriale $V$ è un insieme dotato di una somma "$+$", sotto cui è un gruppo abeliano, e di una moltiplicazione con elementi di un campo $C$ compatibile con la somma di cui sopra. 
\end{definition}

Quindi dati $x,y \in V$ e $\lambda, \mu \in C$ vale che:

\begin{enumerate}
\item $x+y = y+x \in V$
\item $\lambda x \in V$
\item $\lambda(x+y) = \lambda x + \lambda y$
\item $(\lambda + \mu)x = \lambda x + \mu x$
\item $(\lambda \mu)x = \lambda (\mu x)$
\end{enumerate} 

Inoltre, dati $0,1 \in C$ allora $0 \cdot x = 0$ e $1 \cdot x = x$.

Per noi il campo $C$ sarà sempre uno tra il campo dei numeri reali $\RR$ e quello dei numeri complessi $\CC$. Gli esempi più semplici di spazi vettoriali sono le n-ple di numeri. $\RR^n$ definito da tutti gli elementi del tipo $(x_1, x_2, \ldots , x_n)$ con $x_i \in \RR$, dove:

\begin{itemize}
\item $(x_1, x_2, \ldots , x_n) + (y_1, y_2, \ldots , y_n) = (x_1 + y_1, x_2 + y_2,\ldots, x_n + y_n)$;
\item $\lambda (x_1, x_2, \ldots , x_n) = (\lambda x_1,\lambda x_2, \ldots , \lambda x_n)$.
\end{itemize} 

Analogamente si può definire $\CC^n$.\\

Spesso (ma non sempre!) le "soluzioni di un problema" matematico o fisico formano uno spazio vettoriale. Ad esempio le soluzioni di un'equazione differenziale lineare come

\begin{equation}
\alpha(x) u''(x) + \beta(x) u'(x) + \gamma(x) u(x) = 0
\end{equation}
 
in cui, se $u_1(x)$ e $u_2$ sono soluzioni, anche $\lambda u_1(x) + \mu u_2(x)$ lo è, per ogni scelta di $\lambda, \mu \in \RR$ (o $\CC$). In questo caso si dice che il problema è lineare e soddisfa il "principio di sovrapposizione". Capita spesso che un problema generico (quindi non lineare) diventi lineare nell'approssimazione di piccole fluttuazioni attorno ad un punto di equilibrio. Oppure ci sono casi in cui il principio di sovrapposizione vale in generale come in Elettromagnetismo o in Meccanica Quantistica.

\begin{definition}{(lineare indipendenza)}
Dati $n$ elementi $x_1, x_2, \ldots, x_n$ una generica combinazione lineare è $\lambda_1 x_1 + \lambda_2 x_2 + \cdots + \lambda_n x_n$. Se $\lambda_1 x_1 + \cdots \lambda_n x_n = 0$ implica $\lambda_i = 0 \forall i$ allora i vettori $x_i$ sono detti linearmente indipendenti. 
\end{definition}

\begin{definition}{(dimensione di uno spazio)}
La dimensione di uno spazio $\dim{V}$ è definita dal numero massimo di vettori linearmente indipendenti che si trovano in esso. Se $\dim{V} = n$ con $n \in \NN$ allora lo spazio ha dimensione finita $n$. Se $\forall n \in \NN$ si possono trovare $n$ vettori linearmente indipendenti, allora si dice che lo spazio ha dimensione infinita.   
\end{definition}

Per ora ci limitiamo a ripassare alcune nozioni fondamentali sugli spazi a dimensione finita $\dim{V} = n$.

\begin{definition}{(base di uno spazio)}
Una base $y_1, y_2, \ldots , y_m$ di $V$ è un insieme di vettori linearmente indipendenti che genera $V$, cioè che ogni $x \in V$ può essere scritto come $x = \lambda_1 y_1 + \lambda_2 y_2 + \cdots + \lambda_m y_m$.
\end{definition}

\begin{theorem}
Sia $V$ uno spazio vettoriale a dimensione finita con $\dim{V} = n$. Sia $y_1, y_2, \ldots , y_m$ una base di $V$, allora $m=n$.
\end{theorem}

\begin{proof}
È ovvio che $m \leq n$ per definizione. Prendiamo $x_1, x_2, \ldots, x_n$ un insieme di vettori linearmente indipendenti. Dato che 
\begin{equation*}
x_1 = \lambda_1^{(1)} y_1 + \lambda_2^{(1)} y_2 + \cdots + \lambda_m^{(1)} y_m
\end{equation*}
ci sarà almeno un $i$ per cui $\lambda_i^{(1)} \neq 0$, per cui (senza perdita di generalità)
\begin{equation*}
y_1 = \frac{x_1}{\lambda_1^{(1)}} - \frac{\lambda_2^{(1)}}{\lambda_1^{(1)}} y_2 - \cdots - \frac{\lambda_m^{(1)}}{\lambda_1^{(1)}} y_m.
\end{equation*} 
Quindi $x_1, y_2, \ldots , y_m$ sono una nuova base per $V$. A questo punto si può iterare questo procedimento fino a creare una nuova base di $m$ vettori scelti fra gli $\{x_i\}$.Ma allora deve essere per forza $m=n$. 
\end{proof}

Se prendiamo $\RR^n$ una base è $e_i = (0, \ldots, 1, \ldots,0)$ con $1$ nella i-esima posizione. Infatti sono linearmente indipendenti e $\forall x \in \RR^n$ 
\begin{equation*}
(x_1, x_2, \ldots, x_n) = e_1 x_1 + e_2 x_2 + \cdots e_n x_n
\end{equation*}
Quindi generano $\RR^n$. Quindi $\dim{\RR^n} = n$. Se prendiamo $\CC^n$ la sua dimensione come spazio vettoriale come spazio vettoriale sul campo $\CC$ è $\dim_\CC{\CC^n} = n$. In questo caso omettiamo il $ \CC$ e scrivere semplicemente $\dim{\CC^n} =  n$. Come spazio vettoriale nel campo dei reali invece $\dim_\RR{\CC^n} = 2n$, dove una possibile base è $e_j^1 = (0,\ldots,1,\ldots,0), \; e_j^2 = (0,\ldots,i,\ldots,0)$ con $1 \geq j \leq n$. 

\begin{definition}{(sottospazio vettoriale)}
Se abbiamo $m<n$ vettori linearmente indipendenti $y_1, y_2, \ldots , y_m$ in uno spazio vettoriale $V$ con $\dim	{V} = n$, allora l'insieme di tutte le combinazioni lineari $\lambda_1 y_1 + \lambda_2 y_2 + \cdots + \lambda_m y_m$ è un sottospazio vettoriale $V' \subset V$ con $\dim{V'}$.
\end{definition}

Data una base $x_1, x_2, \ldots, x_n$ di elementi di $V$ possiamo fare un isomorfismo fra $V$ e $\RR^n$. Dato un $x \in V$ lo scriviamo in modo univoco come $x = $ ed associamo 
\begin{equation*}
x \in V \longleftrightarrow (\lambda_1, \lambda_2, \ldots, \lambda_n) \in \RR^n.
\end{equation*}
Tutti gli spazi vettoriali di dimensione finita su $\RR$ ($\CC$) sono isomorfi a $\RR^n$ ($\CC^n$) dove $n$ è la loro dimensione. Ovviamente l'isomorfismo richiede la scelta preliminare di una base.

\begin{definition}{(applicazione lineare)}
Dati due spazi vettoriali $V$, $\dim{V} = n$, e $W$, $\dim{W} = m$, un'applicazione lineare è una funzione 
\begin{align*}
A : V \longrightarrow W
\end{align*} 
che rispetta la struttura lineare
\begin{equation*}
A(\lambda_1 x_1 + \lambda_2 x_2 + \cdots + \lambda_n x_n) = \lambda_1 A(x_1) + \lambda_2 A(x_2) + \cdots + \lambda_n A(x_n).
\end{equation*}
\end{definition}
\begin{proposition}
Sia $e_1, e_2, \ldots, e_n$ una base di uno spazio vettoriale $V$. La conoscenza di $A$ sugli elementi della base implica la conoscenza di $A$ su tutto lo spazio.
\end{proposition}

\begin{proof}
Se $x = \lambda_1 e_1 + \lambda_2 e_2 + \cdots + \lambda_n e_n \; \forall x \in V$, allora $A(x) = A(\lambda_1 e_1 + \lambda_2 e_2 + \cdots + \lambda_n e_n) = \lambda_1 A(e_1) + \lambda_2 A(e_2) + \cdots + \lambda_n (e_n)$.
\end{proof} 
Se prendiamo una base $e_1, e_2, \ldots, e_n$ di $V$ e una base $f_1, f_2, \ldots, f_m$ di $W$, allora possiamo tradurre $A$ in un'applicazione lineare fra $\RR^n \rightarrow \RR^m$ quindi scriverla in forma matriciale
\begin{equation*}
\begin{pmatrix}
\mu_1 \\
\mu_2 \\
\vdots \\
\mu_m
\end{pmatrix}
=
\begin{pmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{m,1} & a_{m,2} & \cdots & a_{m,n}
\end{pmatrix}
\begin{pmatrix}
\lambda_1 \\
\lambda_2 \\
\vdots \\
\lambda_n
\end{pmatrix}
\end{equation*}
Ogni $A(e_i)$ può essere scomposto nella base $\{f_j\}$
\begin{equation*}
A(e_i) = \sum_j{a_{j,i} f_j} \\
\end{equation*}
Quindi
\begin{equation*}
A(x) = \sum_i{\lambda_i A(e_i)} = \sum_i\sum_j{\lambda_i a_{j,i} f_j} = \sum_j{\mu_j f_j}
\end{equation*}
Dal seguente prodotto si ottiene un vettore in $\RR^m$, che corrisponde ad $A(x)$ nella base $\{f_j\}$.